{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "from model import GPTLanguageModel, GPTConfig, Block, LayerNorm\n",
    "config = GPTConfig()\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "vocab_size = 50304\n",
    "n_embd = 6\n",
    "n_head = 2\n",
    "dropout = 0.1\n",
    "device_type = 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "bin_dir = os.path.join(os.path.join(os.getcwd(), \"data\"), \"bin\")\n",
    "\n",
    "train_data = np.memmap(os.path.join(bin_dir, \"train.bin\"), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(bin_dir, \"val.bin\"), dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1:i + block_size + 1]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    if device_type == \"cuda\":\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = nn.Embedding(vocab_size, n_embd)\n",
    "wpe = nn.Embedding(block_size, n_embd)\n",
    "attn_layer = nn.Linear(n_embd, 3 * n_embd, bias=True)\n",
    "proj_layer = nn.Linear(n_embd, n_embd, bias=True)\n",
    "attn_dropout = nn.Dropout(dropout)\n",
    "resid_dropout = nn.Dropout(dropout)\n",
    "dropout_layer = nn.Dropout(0.1)\n",
    "ln_1 = LayerNorm(n_embd, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_batch(\"train\")\n",
    "b,t  = X.size()\n",
    "\n",
    "pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "tok_emb = wte(X)    # tok_emb[0,0,:] - Embedding vector for first token\n",
    "pos_emb = wpe(pos)\n",
    "x = dropout_layer(tok_emb + pos_emb)\n",
    "x = ln_1(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,T,C = x.size()\n",
    "\n",
    "q, k, v = attn_layer(x).split(n_embd, dim=2)\n",
    "k = k.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "q = q.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "v = v.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "# Causal Self-Attention: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "bias = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "att = att.masked_fill(bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "att = Func.softmax(att, dim=-1)\n",
    "att = attn_dropout(att)\n",
    "\n",
    "y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "# Output projection\n",
    "y = resid_dropout(proj_layer(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),   # Word token embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),   # Positional Encoding Embedding\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            layer_norm = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "\n",
    "lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_batch(\"train\")\n",
    "\n",
    "def forward(idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= config.block_size, f\"Cannot forward sequence of length {t}, block size is only {config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward the GPT model\n",
    "        tok_emb = transformer.wte(idx)\n",
    "        pos_emb = transformer.wpe(pos)\n",
    "        x = transformer.dropout(tok_emb + pos_emb)\n",
    "        for block in transformer.blocks:\n",
    "            x = block(x)\n",
    "        x = transformer.layer_norm(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = lm_head(x)\n",
    "            loss = Func.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Only forward the lm_head on the very last position\n",
    "            logits = lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, _ = forward(X)\n",
    "logits = logits[:, -1, :]\n",
    "logits.size()\n",
    "probs = Func.softmax(logits, dim=-1)\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "#idx = torch.cat((idx, idx_next), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GPTLanguageModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import TrainConfig\n",
    "con = TrainConfig()\n",
    "con.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import TrainConfig\n",
    "config = TrainConfig()\n",
    "config.max_iters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def run(self):\n",
    "        while self.config.iter_num < self.config.max_iters:\n",
    "            self.config.iter_num += 1\n",
    "            print(self.config.iter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['block_size',\n",
       " 'batch_size',\n",
       " 'vocab_size',\n",
       " 'n_embd',\n",
       " 'n_head',\n",
       " 'dropout',\n",
       " 'device_type',\n",
       " 'device',\n",
       " 'bin_dir']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = [k for k,v in globals().items() if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))]\n",
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
